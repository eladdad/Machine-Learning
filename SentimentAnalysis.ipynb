{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Team 63.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "F-ROTsbzfLEp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "metadata": {
        "id": "ruCdWYszME24",
        "colab_type": "code",
        "outputId": "7848aacd-bc44-448e-ee0a-f82a64b8bb19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "te1Tw25iMFkh",
        "colab_type": "code",
        "outputId": "fa8e9160-7cf4-495f-9508-066742c80572",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4326
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> all\n",
            "    Downloading collection 'all'\n",
            "       | \n",
            "       | Downloading package abc to /root/nltk_data...\n",
            "       |   Package abc is already up-to-date!\n",
            "       | Downloading package alpino to /root/nltk_data...\n",
            "       |   Package alpino is already up-to-date!\n",
            "       | Downloading package biocreative_ppi to /root/nltk_data...\n",
            "       |   Package biocreative_ppi is already up-to-date!\n",
            "       | Downloading package brown to /root/nltk_data...\n",
            "       |   Package brown is already up-to-date!\n",
            "       | Downloading package brown_tei to /root/nltk_data...\n",
            "       |   Package brown_tei is already up-to-date!\n",
            "       | Downloading package cess_cat to /root/nltk_data...\n",
            "       |   Package cess_cat is already up-to-date!\n",
            "       | Downloading package cess_esp to /root/nltk_data...\n",
            "       |   Package cess_esp is already up-to-date!\n",
            "       | Downloading package chat80 to /root/nltk_data...\n",
            "       |   Package chat80 is already up-to-date!\n",
            "       | Downloading package city_database to /root/nltk_data...\n",
            "       |   Package city_database is already up-to-date!\n",
            "       | Downloading package cmudict to /root/nltk_data...\n",
            "       |   Package cmudict is already up-to-date!\n",
            "       | Downloading package comparative_sentences to\n",
            "       |     /root/nltk_data...\n",
            "       |   Package comparative_sentences is already up-to-date!\n",
            "       | Downloading package comtrans to /root/nltk_data...\n",
            "       |   Package comtrans is already up-to-date!\n",
            "       | Downloading package conll2000 to /root/nltk_data...\n",
            "       |   Package conll2000 is already up-to-date!\n",
            "       | Downloading package conll2002 to /root/nltk_data...\n",
            "       |   Package conll2002 is already up-to-date!\n",
            "       | Downloading package conll2007 to /root/nltk_data...\n",
            "       |   Package conll2007 is already up-to-date!\n",
            "       | Downloading package crubadan to /root/nltk_data...\n",
            "       |   Package crubadan is already up-to-date!\n",
            "       | Downloading package dependency_treebank to /root/nltk_data...\n",
            "       |   Package dependency_treebank is already up-to-date!\n",
            "       | Downloading package dolch to /root/nltk_data...\n",
            "       |   Package dolch is already up-to-date!\n",
            "       | Downloading package europarl_raw to /root/nltk_data...\n",
            "       |   Package europarl_raw is already up-to-date!\n",
            "       | Downloading package floresta to /root/nltk_data...\n",
            "       |   Package floresta is already up-to-date!\n",
            "       | Downloading package framenet_v15 to /root/nltk_data...\n",
            "       |   Package framenet_v15 is already up-to-date!\n",
            "       | Downloading package framenet_v17 to /root/nltk_data...\n",
            "       |   Package framenet_v17 is already up-to-date!\n",
            "       | Downloading package gazetteers to /root/nltk_data...\n",
            "       |   Package gazetteers is already up-to-date!\n",
            "       | Downloading package genesis to /root/nltk_data...\n",
            "       |   Package genesis is already up-to-date!\n",
            "       | Downloading package gutenberg to /root/nltk_data...\n",
            "       |   Package gutenberg is already up-to-date!\n",
            "       | Downloading package ieer to /root/nltk_data...\n",
            "       |   Package ieer is already up-to-date!\n",
            "       | Downloading package inaugural to /root/nltk_data...\n",
            "       |   Package inaugural is already up-to-date!\n",
            "       | Downloading package indian to /root/nltk_data...\n",
            "       |   Package indian is already up-to-date!\n",
            "       | Downloading package jeita to /root/nltk_data...\n",
            "       |   Package jeita is already up-to-date!\n",
            "       | Downloading package kimmo to /root/nltk_data...\n",
            "       |   Package kimmo is already up-to-date!\n",
            "       | Downloading package knbc to /root/nltk_data...\n",
            "       |   Package knbc is already up-to-date!\n",
            "       | Downloading package lin_thesaurus to /root/nltk_data...\n",
            "       |   Package lin_thesaurus is already up-to-date!\n",
            "       | Downloading package mac_morpho to /root/nltk_data...\n",
            "       |   Package mac_morpho is already up-to-date!\n",
            "       | Downloading package machado to /root/nltk_data...\n",
            "       |   Package machado is already up-to-date!\n",
            "       | Downloading package masc_tagged to /root/nltk_data...\n",
            "       |   Package masc_tagged is already up-to-date!\n",
            "       | Downloading package moses_sample to /root/nltk_data...\n",
            "       |   Package moses_sample is already up-to-date!\n",
            "       | Downloading package movie_reviews to /root/nltk_data...\n",
            "       |   Package movie_reviews is already up-to-date!\n",
            "       | Downloading package names to /root/nltk_data...\n",
            "       |   Package names is already up-to-date!\n",
            "       | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "       |   Package nombank.1.0 is already up-to-date!\n",
            "       | Downloading package nps_chat to /root/nltk_data...\n",
            "       |   Package nps_chat is already up-to-date!\n",
            "       | Downloading package omw to /root/nltk_data...\n",
            "       |   Package omw is already up-to-date!\n",
            "       | Downloading package opinion_lexicon to /root/nltk_data...\n",
            "       |   Package opinion_lexicon is already up-to-date!\n",
            "       | Downloading package paradigms to /root/nltk_data...\n",
            "       |   Package paradigms is already up-to-date!\n",
            "       | Downloading package pil to /root/nltk_data...\n",
            "       |   Package pil is already up-to-date!\n",
            "       | Downloading package pl196x to /root/nltk_data...\n",
            "       |   Package pl196x is already up-to-date!\n",
            "       | Downloading package ppattach to /root/nltk_data...\n",
            "       |   Package ppattach is already up-to-date!\n",
            "       | Downloading package problem_reports to /root/nltk_data...\n",
            "       |   Package problem_reports is already up-to-date!\n",
            "       | Downloading package propbank to /root/nltk_data...\n",
            "       |   Package propbank is already up-to-date!\n",
            "       | Downloading package ptb to /root/nltk_data...\n",
            "       |   Package ptb is already up-to-date!\n",
            "       | Downloading package product_reviews_1 to /root/nltk_data...\n",
            "       |   Package product_reviews_1 is already up-to-date!\n",
            "       | Downloading package product_reviews_2 to /root/nltk_data...\n",
            "       |   Package product_reviews_2 is already up-to-date!\n",
            "       | Downloading package pros_cons to /root/nltk_data...\n",
            "       |   Package pros_cons is already up-to-date!\n",
            "       | Downloading package qc to /root/nltk_data...\n",
            "       |   Package qc is already up-to-date!\n",
            "       | Downloading package reuters to /root/nltk_data...\n",
            "       |   Package reuters is already up-to-date!\n",
            "       | Downloading package rte to /root/nltk_data...\n",
            "       |   Package rte is already up-to-date!\n",
            "       | Downloading package semcor to /root/nltk_data...\n",
            "       |   Package semcor is already up-to-date!\n",
            "       | Downloading package senseval to /root/nltk_data...\n",
            "       |   Package senseval is already up-to-date!\n",
            "       | Downloading package sentiwordnet to /root/nltk_data...\n",
            "       |   Package sentiwordnet is already up-to-date!\n",
            "       | Downloading package sentence_polarity to /root/nltk_data...\n",
            "       |   Package sentence_polarity is already up-to-date!\n",
            "       | Downloading package shakespeare to /root/nltk_data...\n",
            "       |   Package shakespeare is already up-to-date!\n",
            "       | Downloading package sinica_treebank to /root/nltk_data...\n",
            "       |   Package sinica_treebank is already up-to-date!\n",
            "       | Downloading package smultron to /root/nltk_data...\n",
            "       |   Package smultron is already up-to-date!\n",
            "       | Downloading package state_union to /root/nltk_data...\n",
            "       |   Package state_union is already up-to-date!\n",
            "       | Downloading package stopwords to /root/nltk_data...\n",
            "       |   Package stopwords is already up-to-date!\n",
            "       | Downloading package subjectivity to /root/nltk_data...\n",
            "       |   Package subjectivity is already up-to-date!\n",
            "       | Downloading package swadesh to /root/nltk_data...\n",
            "       |   Package swadesh is already up-to-date!\n",
            "       | Downloading package switchboard to /root/nltk_data...\n",
            "       |   Package switchboard is already up-to-date!\n",
            "       | Downloading package timit to /root/nltk_data...\n",
            "       |   Package timit is already up-to-date!\n",
            "       | Downloading package toolbox to /root/nltk_data...\n",
            "       |   Package toolbox is already up-to-date!\n",
            "       | Downloading package treebank to /root/nltk_data...\n",
            "       |   Package treebank is already up-to-date!\n",
            "       | Downloading package twitter_samples to /root/nltk_data...\n",
            "       |   Package twitter_samples is already up-to-date!\n",
            "       | Downloading package udhr to /root/nltk_data...\n",
            "       |   Package udhr is already up-to-date!\n",
            "       | Downloading package udhr2 to /root/nltk_data...\n",
            "       |   Package udhr2 is already up-to-date!\n",
            "       | Downloading package unicode_samples to /root/nltk_data...\n",
            "       |   Package unicode_samples is already up-to-date!\n",
            "       | Downloading package universal_treebanks_v20 to\n",
            "       |     /root/nltk_data...\n",
            "       |   Package universal_treebanks_v20 is already up-to-date!\n",
            "       | Downloading package verbnet to /root/nltk_data...\n",
            "       |   Package verbnet is already up-to-date!\n",
            "       | Downloading package verbnet3 to /root/nltk_data...\n",
            "       |   Package verbnet3 is already up-to-date!\n",
            "       | Downloading package webtext to /root/nltk_data...\n",
            "       |   Package webtext is already up-to-date!\n",
            "       | Downloading package wordnet to /root/nltk_data...\n",
            "       |   Package wordnet is already up-to-date!\n",
            "       | Downloading package wordnet_ic to /root/nltk_data...\n",
            "       |   Package wordnet_ic is already up-to-date!\n",
            "       | Downloading package words to /root/nltk_data...\n",
            "       |   Package words is already up-to-date!\n",
            "       | Downloading package ycoe to /root/nltk_data...\n",
            "       |   Package ycoe is already up-to-date!\n",
            "       | Downloading package rslp to /root/nltk_data...\n",
            "       |   Package rslp is already up-to-date!\n",
            "       | Downloading package maxent_treebank_pos_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Package maxent_treebank_pos_tagger is already up-to-date!\n",
            "       | Downloading package universal_tagset to /root/nltk_data...\n",
            "       |   Package universal_tagset is already up-to-date!\n",
            "       | Downloading package maxent_ne_chunker to /root/nltk_data...\n",
            "       |   Package maxent_ne_chunker is already up-to-date!\n",
            "       | Downloading package punkt to /root/nltk_data...\n",
            "       |   Package punkt is already up-to-date!\n",
            "       | Downloading package book_grammars to /root/nltk_data...\n",
            "       |   Package book_grammars is already up-to-date!\n",
            "       | Downloading package sample_grammars to /root/nltk_data...\n",
            "       |   Package sample_grammars is already up-to-date!\n",
            "       | Downloading package spanish_grammars to /root/nltk_data...\n",
            "       |   Package spanish_grammars is already up-to-date!\n",
            "       | Downloading package basque_grammars to /root/nltk_data...\n",
            "       |   Package basque_grammars is already up-to-date!\n",
            "       | Downloading package large_grammars to /root/nltk_data...\n",
            "       |   Package large_grammars is already up-to-date!\n",
            "       | Downloading package tagsets to /root/nltk_data...\n",
            "       |   Package tagsets is already up-to-date!\n",
            "       | Downloading package snowball_data to /root/nltk_data...\n",
            "       |   Package snowball_data is already up-to-date!\n",
            "       | Downloading package bllip_wsj_no_aux to /root/nltk_data...\n",
            "       |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "       | Downloading package word2vec_sample to /root/nltk_data...\n",
            "       |   Package word2vec_sample is already up-to-date!\n",
            "       | Downloading package panlex_swadesh to /root/nltk_data...\n",
            "       |   Package panlex_swadesh is already up-to-date!\n",
            "       | Downloading package mte_teip5 to /root/nltk_data...\n",
            "       |   Package mte_teip5 is already up-to-date!\n",
            "       | Downloading package averaged_perceptron_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Package averaged_perceptron_tagger is already up-to-date!\n",
            "       | Downloading package averaged_perceptron_tagger_ru to\n",
            "       |     /root/nltk_data...\n",
            "       |   Package averaged_perceptron_tagger_ru is already up-to-\n",
            "       |       date!\n",
            "       | Downloading package perluniprops to /root/nltk_data...\n",
            "       |   Package perluniprops is already up-to-date!\n",
            "       | Downloading package nonbreaking_prefixes to\n",
            "       |     /root/nltk_data...\n",
            "       |   Package nonbreaking_prefixes is already up-to-date!\n",
            "       | Downloading package vader_lexicon to /root/nltk_data...\n",
            "       |   Package vader_lexicon is already up-to-date!\n",
            "       | Downloading package porter_test to /root/nltk_data...\n",
            "       |   Package porter_test is already up-to-date!\n",
            "       | Downloading package wmt15_eval to /root/nltk_data...\n",
            "       |   Package wmt15_eval is already up-to-date!\n",
            "       | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "       |   Package mwa_ppdb is already up-to-date!\n",
            "       | \n",
            "     Done downloading collection all\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "OlRiFGrzMKnH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.stem.snowball import SnowballStemmer as stem\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from sklearn.metrics.pairwise import cosine_similarity as cos_sim\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wW2-CVOQiP3t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Read Dataset"
      ]
    },
    {
      "metadata": {
        "id": "fW5RCeO1MM24",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tweets = pd.read_csv('gdrive/My Drive/Datasets/Tweets.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zpVgrvNTlnsD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Filteration"
      ]
    },
    {
      "metadata": {
        "id": "CioKu18ulmcx",
        "colab_type": "code",
        "outputId": "63bc12d4-f8eb-4406-ac29-f1e3dc257037",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Number of samples before filteration : \", tweets.shape[0])\n",
        "\n",
        "idx = []\n",
        "for i,t in tweets['text'].iteritems():\n",
        "  if (\"RT\" in t) or (len(t)<20):\n",
        "    idx.append(i)\n",
        "tweets = tweets.drop(idx)\n",
        "\n",
        "print(\"Number of samples after filteration : \", tweets.shape[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples before filteration :  14640\n",
            "Number of samples after filteration :  14365\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tpMFfuDBnIiY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "text = tweets['text']\n",
        "y = tweets['airline_sentiment']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RCqhYvgCidxN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Normalize"
      ]
    },
    {
      "metadata": {
        "id": "CgqY1YX3MWtP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "stemmer = stem('english')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "punct = ['.',',']\n",
        "#Tokens = \n",
        "text = text.apply(lambda x: ' '.join([stemmer.stem(re.sub(r'http\\S+', '', word.lower())) for word in TweetTokenizer(strip_handles=True).tokenize(x) if (word[0] not in punct and word.lower() not in stop_words)]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T3rK7U9WRBCr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Classification"
      ]
    },
    {
      "metadata": {
        "id": "q77D4HrPhypk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Uz8HupJARG7Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Splitting data to training and testing"
      ]
    },
    {
      "metadata": {
        "id": "hNuW_5j3Q2Zz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(text, y, test_size=0.2, random_state=0)\n",
        "scores = []\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "X_test = vectorizer.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3f1cIU79Q3MZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Multinomial Naive Bayes Classifier"
      ]
    },
    {
      "metadata": {
        "id": "45EMO7UEQuaQ",
        "colab_type": "code",
        "outputId": "f3358197-bafe-45f0-eae6-1565943e6c0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "clf = GridSearchCV(\n",
        "    MultinomialNB(),\n",
        "    param_grid = {'alpha': np.logspace(-2., 2., 50)})\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "scores.append(f1_score(y_test, y_pred, average=\"micro\"))\n",
        "\n",
        "print(clf.best_estimator_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MultinomialNB(alpha=0.1151395399326447, class_prior=None, fit_prior=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xeJgWfc5Zl98",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## K Nearest Neighbors Classifier"
      ]
    },
    {
      "metadata": {
        "id": "o0qsjMnTPbRE",
        "colab_type": "code",
        "outputId": "0e2fdc80-12ee-479b-e158-dfc75d572aee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "neigh = GridSearchCV(\n",
        "    KNeighborsClassifier(),\n",
        "    param_grid = {'n_neighbors': [15, 20, 30 ,35, 40]})\n",
        "neigh.fit(X_train, y_train)\n",
        "y_pred = neigh.predict(X_test)\n",
        "scores.append(f1_score(y_test, y_pred, average=\"micro\"))\n",
        "\n",
        "print(neigh.best_estimator_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
            "           metric_params=None, n_jobs=1, n_neighbors=40, p=2,\n",
            "           weights='uniform')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lkaGvmbwZsCy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Random Forest Classifier"
      ]
    },
    {
      "metadata": {
        "id": "rjME-Az8ZsXj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "clf = RandomForestClassifier(n_estimators = 100, max_depth = 400, random_state = 0)\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "scores.append(f1_score(y_test, y_pred, average=\"micro\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GALpU6zwangV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Scores"
      ]
    },
    {
      "metadata": {
        "id": "JoruRQZ1aVrz",
        "colab_type": "code",
        "outputId": "ed4dc1a3-e25d-4e39-edc9-68a70cbf4d9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Multinomial Naive Bayes Classifier F1 Score =\", scores[0])\n",
        "print(\"K Nearest Neighbors Classifier F1 Score =\", scores[1])\n",
        "print(\"Random Forest Classifier F1 Score =\", scores[2])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Multinomial Naive Bayes Classifier F1 Score = 0.7473024712843718\n",
            "K Nearest Neighbors Classifier F1 Score = 0.7229376957883745\n",
            "Random Forest Classifier F1 Score = 0.7741037243299687\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}